#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸…ç†å°å‡ºæª”æ¡ˆä¸­çš„é‡è¤‡è³‡æ–™
"""

import pandas as pd
import os
from datetime import datetime

class ExportDataCleaner:
    def __init__(self, file_path):
        self.file_path = file_path
        self.df = None
        
    def load_data(self):
        """è¼‰å…¥è³‡æ–™"""
        try:
            self.df = pd.read_csv(self.file_path, encoding='utf-8')
            print(f"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™: {len(self.df)} ç­†è¨˜éŒ„")
            return True
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            return False
    
    def analyze_duplicates(self):
        """åˆ†æé‡è¤‡è³‡æ–™"""
        if self.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥è³‡æ–™")
            return
        
        print("\nğŸ” åˆ†æé‡è¤‡è³‡æ–™...")
        
        # 1. å®Œå…¨é‡è¤‡çš„è¨˜éŒ„
        print("\n1. å®Œå…¨é‡è¤‡çš„è¨˜éŒ„:")
        complete_duplicates = self.df.duplicated()
        complete_dup_count = complete_duplicates.sum()
        print(f"   å®Œå…¨é‡è¤‡è¨˜éŒ„: {complete_dup_count} ç­†")
        
        if complete_dup_count > 0:
            print("   å®Œå…¨é‡è¤‡è¨˜éŒ„ç¯„ä¾‹:")
            dup_records = self.df[complete_duplicates].head(3)
            for i, (_, row) in enumerate(dup_records.iterrows()):
                print(f"     {i+1}. {row.get('æ—¥æœŸ', 'N/A')} - {row.get('EPAé …ç›®', 'N/A')} - {row.get('å€‹æ¡ˆå§“å', 'N/A')}")
        
        # 2. åŸºæ–¼é—œéµæ¬„ä½çš„é‡è¤‡ï¼ˆé™¤äº†è³‡æ–™ä¾†æºï¼‰
        print("\n2. åŸºæ–¼é—œéµæ¬„ä½çš„é‡è¤‡è¨˜éŒ„:")
        key_columns = ['æ—¥æœŸ', 'EPAé …ç›®', 'ç—…æ­·è™Ÿç¢¼', 'å€‹æ¡ˆå§“å', 'è¨ºæ–·']
        
        # æ¨™æº–åŒ–ç—…æ­·è™Ÿç¢¼ï¼ˆç§»é™¤.0å¾Œç¶´ï¼‰
        df_temp = self.df.copy()
        df_temp['ç—…æ­·è™Ÿç¢¼_æ¨™æº–åŒ–'] = df_temp['ç—…æ­·è™Ÿç¢¼'].astype(str).str.replace('.0', '')
        
        # å‰µå»ºåˆä½µéµ
        df_temp['merge_key'] = df_temp[['æ—¥æœŸ', 'EPAé …ç›®', 'ç—…æ­·è™Ÿç¢¼_æ¨™æº–åŒ–', 'å€‹æ¡ˆå§“å', 'è¨ºæ–·']].astype(str).agg('|'.join, axis=1)
        
        # æ‰¾å‡ºé‡è¤‡
        key_duplicates = df_temp.duplicated(subset=['merge_key'], keep=False)
        key_dup_count = key_duplicates.sum()
        print(f"   é—œéµæ¬„ä½é‡è¤‡è¨˜éŒ„: {key_dup_count} ç­†")
        
        if key_dup_count > 0:
            print("   é—œéµæ¬„ä½é‡è¤‡è¨˜éŒ„ç¯„ä¾‹:")
            key_dup_records = df_temp[key_duplicates].head(6)
            for i, (_, row) in enumerate(key_dup_records.iterrows()):
                print(f"     {i+1}. {row.get('æ—¥æœŸ', 'N/A')} - {row.get('EPAé …ç›®', 'N/A')} - {row.get('å€‹æ¡ˆå§“å', 'N/A')} - {row.get('è³‡æ–™ä¾†æº', 'N/A')}")
        
        # 3. è³‡æ–™ä¾†æºåˆ†å¸ƒ
        print("\n3. è³‡æ–™ä¾†æºåˆ†å¸ƒ:")
        if 'è³‡æ–™ä¾†æº' in self.df.columns:
            source_counts = self.df['è³‡æ–™ä¾†æº'].value_counts()
            for source, count in source_counts.items():
                percentage = (count / len(self.df)) * 100
                print(f"   {source}: {count} ç­† ({percentage:.1f}%)")
        
        return complete_dup_count, key_dup_count
    
    def clean_data(self):
        """æ¸…ç†é‡è¤‡è³‡æ–™"""
        if self.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥è³‡æ–™")
            return None
        
        print("\nğŸ§¹ é–‹å§‹æ¸…ç†é‡è¤‡è³‡æ–™...")
        
        # è¨˜éŒ„åŸå§‹æ•¸é‡
        original_count = len(self.df)
        
        # 1. ç§»é™¤å®Œå…¨é‡è¤‡çš„è¨˜éŒ„
        print("1. ç§»é™¤å®Œå…¨é‡è¤‡çš„è¨˜éŒ„...")
        complete_duplicates = self.df.duplicated()
        complete_dup_count = complete_duplicates.sum()
        self.df = self.df[~complete_duplicates]
        print(f"   ç§»é™¤ {complete_dup_count} ç­†å®Œå…¨é‡è¤‡è¨˜éŒ„")
        
        # 2. è™•ç†åŸºæ–¼é—œéµæ¬„ä½çš„é‡è¤‡
        print("2. è™•ç†åŸºæ–¼é—œéµæ¬„ä½çš„é‡è¤‡è¨˜éŒ„...")
        
        # æ¨™æº–åŒ–ç—…æ­·è™Ÿç¢¼
        self.df['ç—…æ­·è™Ÿç¢¼_æ¨™æº–åŒ–'] = self.df['ç—…æ­·è™Ÿç¢¼'].astype(str).str.replace('.0', '')
        
        # å‰µå»ºåˆä½µéµ
        self.df['merge_key'] = self.df[['æ—¥æœŸ', 'EPAé …ç›®', 'ç—…æ­·è™Ÿç¢¼_æ¨™æº–åŒ–', 'å€‹æ¡ˆå§“å', 'è¨ºæ–·']].astype(str).agg('|'.join, axis=1)
        
        # æ‰¾å‡ºé‡è¤‡è¨˜éŒ„
        key_duplicates = self.df.duplicated(subset=['merge_key'], keep=False)
        key_dup_count = key_duplicates.sum()
        
        if key_dup_count > 0:
            print(f"   ç™¼ç¾ {key_dup_count} ç­†åŸºæ–¼é—œéµæ¬„ä½çš„é‡è¤‡è¨˜éŒ„")
            
            # å„ªå…ˆä¿ç•™ç¾æœ‰ç³»çµ±çš„è³‡æ–™
            duplicate_groups = self.df[key_duplicates].groupby('merge_key')
            
            records_to_remove = []
            for key, group in duplicate_groups:
                if len(group) > 1:
                    # æª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰ç³»çµ±çš„è³‡æ–™
                    current_system_records = group[group['è³‡æ–™ä¾†æº'] == 'ç¾æœ‰ç³»çµ±']
                    
                    if not current_system_records.empty:
                        # å¦‚æœæœ‰ç¾æœ‰ç³»çµ±çš„è³‡æ–™ï¼Œç§»é™¤EMYWAYæ­·å²è³‡æ–™
                        emway_records = group[group['è³‡æ–™ä¾†æº'] == 'EMYWAYæ­·å²è³‡æ–™']
                        if not emway_records.empty:
                            records_to_remove.extend(emway_records.index.tolist())
                            print(f"     ä¿ç•™ç¾æœ‰ç³»çµ±è¨˜éŒ„ï¼Œç§»é™¤EMYWAYè¨˜éŒ„: {key[:50]}...")
                    else:
                        # å¦‚æœåªæœ‰EMYWAYè³‡æ–™ï¼Œä¿ç•™ç¬¬ä¸€ç­†
                        keep_record = group.index[0]
                        remove_records = group.index[1:].tolist()
                        records_to_remove.extend(remove_records)
                        print(f"     åªä¿ç•™ç¬¬ä¸€ç­†EMYWAYè¨˜éŒ„: {key[:50]}...")
            
            # ç§»é™¤é‡è¤‡è¨˜éŒ„
            self.df = self.df.drop(records_to_remove)
            print(f"   ç§»é™¤ {len(records_to_remove)} ç­†é‡è¤‡è¨˜éŒ„")
        
        # 3. æ¸…ç†è‡¨æ™‚æ¬„ä½
        self.df = self.df.drop(['ç—…æ­·è™Ÿç¢¼_æ¨™æº–åŒ–', 'merge_key'], axis=1)
        
        # 4. çµ±è¨ˆæ¸…ç†çµæœ
        final_count = len(self.df)
        removed_count = original_count - final_count
        
        print(f"\nğŸ“Š æ¸…ç†çµæœ:")
        print(f"   åŸå§‹è¨˜éŒ„æ•¸: {original_count} ç­†")
        print(f"   æ¸…ç†å¾Œè¨˜éŒ„æ•¸: {final_count} ç­†")
        print(f"   ç§»é™¤è¨˜éŒ„æ•¸: {removed_count} ç­†")
        print(f"   æ¸…ç†ç‡: {(removed_count / original_count * 100):.1f}%")
        
        return self.df
    
    def save_cleaned_data(self, output_path=None):
        """å„²å­˜æ¸…ç†å¾Œçš„è³‡æ–™"""
        if self.df is None:
            print("âŒ æ²’æœ‰è³‡æ–™å¯å„²å­˜")
            return
        
        if output_path is None:
            # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆåç¨±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(os.path.basename(self.file_path))[0]
            output_path = os.path.join(os.path.dirname(self.file_path), f"{base_name}_cleaned_{timestamp}.csv")
        
        try:
            self.df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"âœ… æ¸…ç†å¾Œçš„è³‡æ–™å·²å„²å­˜è‡³: {output_path}")
            return output_path
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")
            return None
    
    def backup_original(self):
        """å‚™ä»½åŸå§‹æª”æ¡ˆ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"{self.file_path}.backup_{timestamp}"
            
            # è¤‡è£½åŸå§‹æª”æ¡ˆ
            import shutil
            shutil.copy2(self.file_path, backup_path)
            print(f"âœ… åŸå§‹æª”æ¡ˆå·²å‚™ä»½è‡³: {backup_path}")
            return backup_path
        except Exception as e:
            print(f"âŒ å‚™ä»½å¤±æ•—: {e}")
            return None

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("ğŸ§¹ å°å‡ºè³‡æ–™é‡è¤‡æ¸…ç†å·¥å…·")
    print("=" * 60)
    
    # æª”æ¡ˆè·¯å¾‘
    file_path = "/Users/mbpr/Library/Mobile Documents/com~apple~CloudDocs/Python/CBME_python/pages/FAM/2025-10-02T10-29_export.csv"
    
    if not os.path.exists(file_path):
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        return
    
    # åˆå§‹åŒ–æ¸…ç†å™¨
    cleaner = ExportDataCleaner(file_path)
    
    # è¼‰å…¥è³‡æ–™
    if not cleaner.load_data():
        return
    
    # åˆ†æé‡è¤‡è³‡æ–™
    complete_dup, key_dup = cleaner.analyze_duplicates()
    
    # å‚™ä»½åŸå§‹æª”æ¡ˆ
    cleaner.backup_original()
    
    # æ¸…ç†è³‡æ–™
    cleaned_df = cleaner.clean_data()
    
    if cleaned_df is not None:
        # å„²å­˜æ¸…ç†å¾Œçš„è³‡æ–™
        output_path = cleaner.save_cleaned_data()
        
        # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
        print(f"\nğŸ‰ è³‡æ–™æ¸…ç†å®Œæˆï¼")
        print(f"ğŸ“ æ¸…ç†å¾Œçš„æª”æ¡ˆ: {output_path}")
        
        # æœ€çµ‚è³‡æ–™ä¾†æºåˆ†å¸ƒ
        if 'è³‡æ–™ä¾†æº' in cleaned_df.columns:
            print(f"\nğŸ“Š æœ€çµ‚è³‡æ–™ä¾†æºåˆ†å¸ƒ:")
            source_counts = cleaned_df['è³‡æ–™ä¾†æº'].value_counts()
            for source, count in source_counts.items():
                percentage = (count / len(cleaned_df)) * 100
                print(f"   {source}: {count} ç­† ({percentage:.1f}%)")
    
    print("=" * 60)

if __name__ == "__main__":
    main()
